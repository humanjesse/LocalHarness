# Loop Flow Diagrams

## 1. Main Chat Loop Architecture

```
app.zig:run()  [while(true) loop starting at line 1030]
â”‚
â”œâ”€â”€â”€ Config Editor Mode (Lines 1031-1107)
â”‚    â””â”€ If config_editor is Some: Handle editor input, save/cancel
â”‚
â”œâ”€â”€â”€ Tool Executor State Machine (Lines 1109-1287)
â”‚    â”‚
â”‚    â”œâ”€ hasPendingWork() â†’ true
â”‚    â”‚  â”‚
â”‚    â”‚  â””â”€ tick() returns:
â”‚    â”‚     â”œâ”€ show_permission_prompt: Display permission UI
â”‚    â”‚     â”œâ”€ render_requested: Execute tool
â”‚    â”‚     â”œâ”€ iteration_complete: Continue to next streaming
â”‚    â”‚     â””â”€ iteration_limit_reached: Stop looping
â”‚    â”‚
â”‚    â””â”€ hasPendingWork() â†’ false: Skip this section
â”‚
â”œâ”€â”€â”€ Compression Checkpoint (After Tool Execution)
â”‚    â”‚
â”‚    â”œâ”€ Condition: !streaming_active && !tool_executor.hasPendingWork()
â”‚    â”‚
â”‚    â””â”€ Check if compression needed:
â”‚        â”œâ”€ context_tracker.estimated_tokens > 70% threshold?
â”‚        â””â”€ Yes â†’ compressor.compressWithAgent() (inline, synchronous)
â”‚            â”œâ”€ Compress old messages (preserve last 5 pairs)
â”‚            â”œâ”€ Update message history in-place
â”‚            â””â”€ Reset token tracker
â”‚
â”œâ”€â”€â”€ Stream Chunk Processing (Lines 1303-1539)
â”‚    â”‚
â”‚    â”œâ”€ If streaming_active: Process stream chunks (10ms sleep)
â”‚    â”‚  â”‚
â”‚    â”‚  â””â”€ For each chunk:
â”‚    â”‚     â”œâ”€ Accumulate thinking/content
â”‚    â”‚     â”œâ”€ chunk.done?
â”‚    â”‚     â”‚  â”œâ”€ true:  Check for tool calls
â”‚    â”‚     â”‚  â”‚         â”œâ”€ Has tools? â†’ tool_executor.startExecution()
â”‚    â”‚     â”‚  â”‚         â””â”€ No tools? â†’ Check compression checkpoint
â”‚    â”‚     â”‚  â””â”€ false: Update assistant message
â”‚    â”‚
â”‚    â””â”€ Non-blocking input handling (10ms loop)
â”‚
â”œâ”€â”€â”€ Main Render Section (Lines 1541-1593)
â”‚    â”‚
â”‚    â””â”€ If !streaming_active:
â”‚        â”œâ”€ Get terminal size
â”‚        â”œâ”€ Render message history
â”‚        â”œâ”€ Render input field
â”‚        â””â”€ Render taskbar
â”‚
â”œâ”€â”€â”€ Input Handling (Lines 1595-1667)
â”‚    â”‚
â”‚    â”œâ”€ If streaming_active || tool_executor.hasPendingWork():
â”‚    â”‚  â””â”€ Non-blocking read (10ms sleep)
â”‚    â”‚
â”‚    â””â”€ Else (idle):
â”‚        â””â”€ Blocking read (waits for input)
â”‚
â””â”€â”€â”€ Cursor & Viewport Management (Lines 1670-1682)
     â””â”€ Adjust scroll position
     
[LOOP BACK TO TOP]
```

---

## 2. Tool Calling Flow

```
User sends message: sendMessage(text)
â”‚
â”œâ”€ Reset tool_call_depth = 0
â”œâ”€ Reset iteration_count = 0
â”œâ”€ Add user message to history
â””â”€ Call startStreaming()
   â”‚
   â”œâ”€ Create assistant placeholder message
   â”‚
   â”œâ”€ Convert app messages to ollama.ChatMessage format
   â”‚  â”œâ”€ Skip display_only_data messages
   â”‚  â”œâ”€ Messages may contain compressed content (ğŸ’¬ [Compressed] prefix)
   â”‚  â””â”€ Hot context injected before LLM call
   â”‚
   â”œâ”€ Create StreamThreadContext:
   â”‚  â”œâ”€ model = config.model
   â”‚  â”œâ”€ messages = prepared messages
   â”‚  â”œâ”€ tools = self.tools (array of ollama.Tool)
   â”‚  â””â”€ ... other config ...
   â”‚
   â””â”€ Spawn background thread: streamingThreadFn()
      â”‚
      â””â”€ In thread: Get provider capabilities
         â”‚
         â”œâ”€ Capability check:
         â”‚  â”œâ”€ enable_thinking = config.enable_thinking AND caps.supports_thinking
         â”‚  â””â”€ keep_alive = caps.supports_keep_alive ? config.keep_alive : null
         â”‚
         â”œâ”€ Call: llm_provider.chatStream()
         â”‚  â”‚
         â”‚  â”œâ”€ Provider dispatch (union switch):
         â”‚  â”‚  â”‚
         â”‚  â”‚  â”œâ”€ Ollama path:
         â”‚  â”‚  â”‚  â””â”€ OllamaProvider.chatStream()
         â”‚  â”‚  â”‚     â”œâ”€ Build JSON payload with:
         â”‚  â”‚  â”‚     â”‚  â”œâ”€ messages
         â”‚  â”‚  â”‚     â”‚  â”œâ”€ tools (if tools.len > 0)
         â”‚  â”‚  â”‚     â”‚  â”œâ”€ keep_alive (if not null)
         â”‚  â”‚  â”‚     â”‚  â””â”€ num_ctx (if set)
         â”‚  â”‚  â”‚     â””â”€ Stream response using callback
         â”‚  â”‚  â”‚
         â”‚  â”‚  â””â”€ LM Studio path:
         â”‚  â”‚     â””â”€ LMStudioProvider.chatStream()
         â”‚  â”‚        â”œâ”€ Build JSON payload with:
         â”‚  â”‚        â”‚  â”œâ”€ messages
         â”‚  â”‚        â”‚  â”œâ”€ tools (if tools.len > 0)
         â”‚  â”‚        â”‚  â””â”€ OpenAI format parameters
         â”‚  â”‚        â”œâ”€ Parse SSE stream
         â”‚  â”‚        â””â”€ Accumulate tool calls by index
         â”‚  â”‚
         â”‚  â””â”€ For each chunk: Call ChunkCallback.callback()
         â”‚     â”œâ”€ Extract thinking/content/tool_calls
         â”‚     â””â”€ Add to stream_chunks queue
         â”‚
         â””â”€ Stream ends: Add .done chunk

[BACK IN MAIN LOOP]
â”‚
â”œâ”€ Process stream chunks until done
â”‚
â””â”€ When chunk.done:
   â”‚
   â”œâ”€ pending_tool_calls = accumulated tool calls
   â”‚
   â”œâ”€ Check: tool_call_depth < max_tool_depth?
   â”‚  â”‚
   â”‚  â”œâ”€ YES:
   â”‚  â”‚  â”œâ”€ Attach tool_calls to assistant message
   â”‚  â”‚  â”œâ”€ Call: tool_executor.startExecution(tool_calls)
   â”‚  â”‚  â””â”€ Next loop iteration: Execute tools
   â”‚  â”‚     â”‚
   â”‚  â”‚     â”œâ”€ For each tool call:
   â”‚  â”‚     â”‚  â”œâ”€ Check permission
   â”‚  â”‚     â”‚  â”œâ”€ Execute: tools_module.executeToolCall()
   â”‚  â”‚     â”‚  â”œâ”€ Create display message (transparency)
   â”‚  â”‚     â”‚  â”œâ”€ Create tool message (JSON result)
   â”‚  â”‚     â”‚  â””â”€ Add both to messages
   â”‚  â”‚     â”‚
   â”‚  â”‚     â”œâ”€ Increment iteration_count
   â”‚  â”‚     â””â”€ Call startStreaming() again (LOOP BACK)
   â”‚  â”‚
   â”‚  â””â”€ NO: Max depth reached
   â”‚     â””â”€ Show error, stop looping
   â”‚
   â””â”€ NO TOOL CALLS:
      â””â”€ Response complete, check compression checkpoint
```

---

## 3. Compression Checkpoint (Inline, Not Secondary Loop)

```
Main Loop checks after tool execution:
  !streaming_active AND !tool_executor.hasPendingWork()
  â”‚
  â””â”€ Check if compression needed:
     â”‚
     â”œâ”€ context_tracker.estimated_tokens_used > (num_ctx * 0.70)?
     â”‚  â”‚
     â”‚  â”œâ”€ YES: Trigger compression (inline, synchronous)
     â”‚  â”‚  â”‚
     â”‚  â”‚  â””â”€ compressor.compressWithAgent(allocator, messages, tracker, llm_provider, config)
     â”‚  â”‚     â”‚
     â”‚  â”‚     â”œâ”€ Step 1: Build agent context
     â”‚  â”‚     â”‚  â”œâ”€ Load compression agent
     â”‚  â”‚     â”‚  â”œâ”€ Provide 4 specialized tools:
     â”‚  â”‚     â”‚  â”‚  â”œâ”€ get_compression_metadata
     â”‚  â”‚     â”‚  â”‚  â”œâ”€ compress_tool_result
     â”‚  â”‚     â”‚  â”‚  â”œâ”€ compress_conversation_segment
     â”‚  â”‚     â”‚  â”‚  â””â”€ verify_compression_target
     â”‚  â”‚     â”‚  â””â”€ Set capabilities (max 15 iterations, temp 0.7)
     â”‚  â”‚     â”‚
     â”‚  â”‚     â”œâ”€ Step 2: Run compression agent
     â”‚  â”‚     â”‚  â”œâ”€ Agent analyzes conversation history
     â”‚  â”‚     â”‚  â”œâ”€ Calls tools to compress messages
     â”‚  â”‚     â”‚  â”‚  â”œâ”€ Tool results: Use tracked metadata
     â”‚  â”‚     â”‚  â”‚  â”œâ”€ User messages: LLM compress to ~50 tokens
     â”‚  â”‚     â”‚  â”‚  â””â”€ Assistant messages: LLM compress to ~200 tokens
     â”‚  â”‚     â”‚  â””â”€ Preserves last 5 user+assistant pairs (protected)
     â”‚  â”‚     â”‚
     â”‚  â”‚     â”œâ”€ Step 3: Update message history in-place
     â”‚  â”‚     â”‚  â”œâ”€ Replace old messages with compressed versions
     â”‚  â”‚     â”‚  â”œâ”€ Free old message content
     â”‚  â”‚     â”‚  â””â”€ Mark compressed messages with ğŸ’¬ [Compressed] prefix
     â”‚  â”‚     â”‚
     â”‚  â”‚     â”œâ”€ Step 4: Reset token tracker
     â”‚  â”‚     â”‚  â”œâ”€ Recalculate estimated tokens
     â”‚  â”‚     â”‚  â””â”€ Target: reduce from 70% (56k) to 40% (32k)
     â”‚  â”‚     â”‚
     â”‚  â”‚     â””â”€ Return compression stats
     â”‚  â”‚        â”œâ”€ original_message_count
     â”‚  â”‚        â”œâ”€ compressed_message_count
     â”‚  â”‚        â”œâ”€ tool_results_compressed
     â”‚  â”‚        â””â”€ messages_protected
     â”‚  â”‚
     â”‚  â””â”€ NO: Continue to next iteration
     â”‚
     â””â”€ [MAIN LOOP CONTINUES]

Compression Quality:
  â”œâ”€ User messages: Preserve question, intent, technical details
  â”œâ”€ Assistant messages: Preserve explanations, code changes, decisions
  â”œâ”€ Tool results: Use metadata for context-aware summaries
  â””â”€ Protected messages: Last 5 pairs never compressed (recent work safe)
```

---

## 4. Provider Tool Handling Comparison

```
Tool Definition (ollama.Tool):
  {
    type: "function",
    function: {
      name: string,
      description: string,
      parameters: JSON schema string
    }
  }

OLLAMA PATH:
â”œâ”€ Create JSON request:
â”‚  â””â”€ "tools": [
â”‚      {
â”‚        "type": "function",
â”‚        "function": { "name": "...", ... }
â”‚      }
â”‚    ]
â”‚
â”œâ”€ Send to: http://localhost:11434/api/chat
â”‚
â”œâ”€ Parse response:
â”‚  â””â”€ message.tool_calls: [
â”‚      {
â”‚        "id": "...",
â”‚        "function": {
â”‚          "name": "...",
â”‚          "arguments": <JSON value>  â† Can be object or string!
â”‚        }
â”‚      }
â”‚    ]
â”‚
â””â”€ Callback: callback(context, thinking, content, tool_calls)

LM STUDIO PATH:
â”œâ”€ Create JSON request (OpenAI format):
â”‚  â””â”€ "tools": [
â”‚      {
â”‚        "type": "function",
â”‚        "function": { "name": "...", ... }
â”‚      }
â”‚    ]
â”‚
â”œâ”€ Send to: http://localhost:1234/v1/chat/completions
â”‚
â”œâ”€ Parse SSE streaming:
â”‚  â””â”€ chunks with delta.tool_calls: [
â”‚      {
â”‚        "index": 0,
â”‚        "id": "...",
â”‚        "function": {
â”‚          "name": "...streaming...",
â”‚          "arguments": "...streaming..."
â”‚        }
â”‚      }
â”‚    ]
â”‚     
â”‚  â”œâ”€ Accumulate by index (tool_calls stream in pieces)
â”‚  â””â”€ Send complete on finish_reason: "tool_calls"
â”‚
â””â”€ Callback: callback(context, reasoning, content, tool_calls)
   (reasoning = LM Studio's thinking equivalent)

KEY DIFFERENCES:
â€¢ Ollama: Complete tool calls in single message chunk
â€¢ LM Studio: Tool calls stream in pieces by index
â€¢ Ollama: thinking field
â€¢ LM Studio: reasoning field (+ index-based accumulation)
â€¢ Both: Same tool format in requests
â€¢ Both: Same callback interface (after accumulation)
```

---

## 5. Message History Flow with Context Management

```
User Message
  â”‚
  â””â”€ startStreaming()
     â”‚
     â”œâ”€ Get message history (may contain compressed messages)
     â”‚  â”œâ”€ Old messages: Compressed if token usage was high
     â”‚  â”‚  â””â”€ Marked with ğŸ’¬ [Compressed] prefix
     â”‚  â””â”€ Recent messages: Last 5 user+assistant pairs (full, never compressed)
     â”‚
     â”œâ”€ Hot Context Injection (BEFORE LLM):
     â”‚  â”‚
     â”‚  â””â”€ injection.buildWorkflowContext():
     â”‚     â”œâ”€ Files read: List from context_tracker
     â”‚     â”œâ”€ Files modified: List with line ranges
     â”‚     â”œâ”€ Current todos: Active task status
     â”‚     â””â”€ Workflow state: Current user activity
     â”‚
     â””â”€ Send to LLM with full context awareness

LLM sees:
  â”œâ”€ Hot context header (workflow awareness)
  â”œâ”€ Compressed old messages (semantic meaning preserved via LLM compression)
  â””â”€ Full recent messages (last 5 pairs protected)

Benefits:
  â”œâ”€ Context window managed automatically
  â”œâ”€ Recent work never compressed
  â”œâ”€ Semantic meaning preserved (not truncation)
  â””â”€ Workflow awareness via hot injection
```

---

## 6. Tool Executor State Machine

```
Initial State: idle

User sends message with tools requested
â”‚
â””â”€ tool_executor.startExecution(tool_calls)
   â”‚
   â””â”€ State: executing

Main loop's tool_executor.tick() runs:
â”‚
â”œâ”€ Has pending permission request?
â”‚  â”‚
â”‚  â”œâ”€ YES: State â†’ show_permission_prompt
â”‚  â”‚      Main loop shows UI, waits for response
â”‚  â”‚      Next tick: User responds, State â†’ executing
â”‚  â”‚
â”‚  â””â”€ NO: Continue to execution
â”‚
â”œâ”€ Current tool in .executing state?
â”‚  â”‚
â”‚  â”œâ”€ YES: State â†’ render_requested
â”‚  â”‚      Main loop calls executeTool()
â”‚  â”‚      Shows results, adds to message history
â”‚  â”‚      Next tick: Advance to next tool
â”‚  â”‚
â”‚  â””â”€ NO: Check all done
â”‚
â”œâ”€ All tools executed?
â”‚  â”‚
â”‚  â”œâ”€ YES: Check iteration limit
â”‚  â”‚      iteration_count < max_iterations?
â”‚  â”‚      â”‚
â”‚  â”‚      â”œâ”€ YES: State â†’ iteration_complete
â”‚  â”‚      â”‚      Main loop calls startStreaming() again
â”‚  â”‚      â”‚      Next tick: Back to initial state
â”‚  â”‚      â”‚
â”‚  â”‚      â””â”€ NO: State â†’ iteration_limit_reached
â”‚  â”‚           Main loop shows error
â”‚  â”‚           Returns to idle
â”‚  â”‚
â”‚  â””â”€ NO: State â†’ executing (next tool)

State: idle (no pending work)
```

---

## 7. Complete Request/Response Cycle

```
USER ENTERS: "Read the file"
   â”‚
   â”œâ”€ sendMessage("Read the file")
   â”‚  â”œâ”€ Add to messages
   â”‚  â””â”€ startStreaming()
   â”‚
   â”œâ”€ Background thread spawned
   â”‚
   â”œâ”€ Thread: Build request
   â”‚  â”œâ”€ Include messages
   â”‚  â”œâ”€ Include tools array
   â”‚  â””â”€ Include capabilities (think, keep_alive)
   â”‚
   â”œâ”€ Thread: Send to LLM (Ollama or LM Studio)
   â”‚
   â”œâ”€ Thread: Stream response
   â”‚  â””â”€ Callback adds chunks to stream_chunks queue
   â”‚
   â””â”€ Main Loop:
      â”‚
      â”œâ”€ Process stream chunks (real-time)
      â”‚  â””â”€ Update assistant message as chunks arrive
      â”‚
      â”œâ”€ Chunk.done received
      â”‚  â”‚
      â”‚  â”œâ”€ Tool calls detected?
      â”‚  â”‚  â”‚
      â”‚  â”‚  â”œâ”€ YES:
      â”‚  â”‚  â”‚  â”œâ”€ tool_executor.startExecution()
      â”‚  â”‚  â”‚  â”‚
      â”‚  â”‚  â”‚  â””â”€ Next loop iteration:
      â”‚  â”‚  â”‚     â”œâ”€ tool_executor.tick() â†’ render_requested
      â”‚  â”‚  â”‚     â”œâ”€ executeTool(tool_call)
      â”‚  â”‚  â”‚     â”‚  â””â”€ Execute: read_file tool
      â”‚  â”‚  â”‚     â”‚     â”œâ”€ Read file from disk
      â”‚  â”‚  â”‚     â”‚     â”œâ”€ Queue for GraphRAG indexing
      â”‚  â”‚  â”‚     â”‚     â””â”€ Return result
      â”‚  â”‚  â”‚     â”œâ”€ Create display message
      â”‚  â”‚  â”‚     â”œâ”€ Create tool message with JSON
      â”‚  â”‚  â”‚     â””â”€ Add both to history
      â”‚  â”‚  â”‚
      â”‚  â”‚  â”‚  â””â”€ Next loop: startStreaming() again
      â”‚  â”‚  â”‚     (LLM sees file content in messages)
      â”‚  â”‚  â”‚
      â”‚  â”‚  â””â”€ NO:
      â”‚  â”‚     â””â”€ Response complete!
      â”‚  â”‚        â””â”€ Check GraphRAG work
      â”‚  â”‚           â”œâ”€ Queue not empty?
      â”‚  â”‚           â””â”€ app_graphrag.processQueuedFiles()
      â”‚  â”‚              â””â”€ Show UI prompt for indexing options
      â”‚  â”‚
      â”‚  â””â”€ Main rendering loop
      â”‚     â””â”€ Display updated message history
      â”‚
      â””â”€ User presses key
         â””â”€ Handle input or continue

[CYCLE COMPLETE - Ready for next user message]
```

---

## 8. File Locations Quick Reference

```
Core Loop Logic:
â”œâ”€ /home/wassie/Desktop/localharness/app.zig (1021-1683)
â”‚  â””â”€ run() = main chat loop (lines 1021-1683)
â”‚  â””â”€ startStreaming() = prepare and spawn thread (lines 647-756)
â”‚  â””â”€ streamingThreadFn() = background streaming (lines 432-578)
â”‚  â””â”€ sendMessage() = user message entry point (lines 759-792)

Tool Handling:
â”œâ”€ /home/wassie/Desktop/localharness/llm_provider.zig
â”‚  â””â”€ Unified interface + provider dispatch
â””â”€ /home/wassie/Desktop/localharness/ollama.zig
   â””â”€ Ollama-specific tool passing
â””â”€ /home/wassie/Desktop/localharness/lmstudio.zig
   â””â”€ LM Studio-specific tool passing

Context Management:
â”œâ”€ /home/wassie/Desktop/localharness/context_management/tracking.zig
â”‚  â””â”€ ContextTracker: Tracks files, modifications, todos, token usage
â”œâ”€ /home/wassie/Desktop/localharness/context_management/compressor.zig
â”‚  â””â”€ Compression logic and LLM-based summarization
â”œâ”€ /home/wassie/Desktop/localharness/injection.zig
â”‚  â””â”€ Hot context injection before LLM calls
â””â”€ /home/wassie/Desktop/localharness/agents_hardcoded/compression_agent.zig
   â””â”€ Compression agent with specialized tools

Compression Tools:
â”œâ”€ /home/wassie/Desktop/localharness/tools/get_compression_metadata.zig
â”œâ”€ /home/wassie/Desktop/localharness/tools/compress_tool_result.zig
â”œâ”€ /home/wassie/Desktop/localharness/tools/compress_conversation_segment.zig
â””â”€ /home/wassie/Desktop/localharness/tools/verify_compression_target.zig

Tool Execution:
â”œâ”€ /home/wassie/Desktop/localharness/tool_executor.zig
â”‚  â””â”€ State machine for tool execution
â””â”€ /home/wassie/Desktop/localharness/tools.zig
   â””â”€ Tool definitions and registry
```

